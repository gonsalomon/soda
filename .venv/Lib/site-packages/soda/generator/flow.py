from __future__ import annotations

import os

import pandas as pd
import questionary as q
from freshness.detector import FreshnessColumnDetector
from prompt_toolkit.shortcuts.prompt import CompleteStyle
from rich.panel import Panel
from ruamel.yaml import YAML

from soda.generator.adaptors.base_adaptor import BaseAdaptor
from soda.generator.exceptions import InvalidDatasetNameException, NoDatasetException
from soda.generator.global_vars import (
    PREVIOUS_STEP_MESSAGE,
    SKIPPED_STEP_MESSAGE,
    SODA_DIR,
    SODA_PRIMARY_GREEN,
)
from soda.generator.models.config_models import (
    Column,
    Dataset,
    DatasetMetadataConfigs,
    DatasetNameConfigs,
    DatasetsMetadata,
)
from soda.generator.sodacl_generator import SodaCLGenerator
from soda.generator.steps import (
    BaseSuggestion,
    ColumnFilter,
    DuplicateValueCheckSuggestion,
    FreshnessCheckSuggestion,
    MissingValueCheckSuggestion,
    PartitionSuggestion,
    RowCountCheckSuggestion,
    SchemaCheckSuggestion,
    SodaCLRunner,
    SuggestionFilter,
    ValidityCheckSuggestion,
)
from soda.generator.tracking.segment import EventTracker
from soda.generator.utils import (
    get_console,
    get_custom_q_style,
    get_raw_flow_configurations,
    parse_sodacl_jinja_template,
    print_header,
    print_info_panel,
    print_warning_panel,
    read_json,
    write_json,
)


class SuggestionFlow:
    def __init__(
        self,
        event_tracker: EventTracker,
        adaptor: BaseAdaptor,
        sodacl_dir: str,
        dataset_name: str | None,
    ):
        self.event_tracker = event_tracker
        self.adaptor = adaptor
        self.console = get_console()
        self.custom_q_style = get_custom_q_style()
        # TODO: this is out of the flow and is confusing/easy to miss
        self.datasets_metadata = self._get_multiple_dataset_metadata(dataset_name=dataset_name)
        self.dataset_name = self._get_dataset_name(dataset_name)
        self.dataset_metadata: Dataset = self._get_single_dataset_metadata(dataset_name=self.dataset_name)
        self.soda_cl_generator = SodaCLGenerator(sodacl_dir=sodacl_dir, dataset_name=self.dataset_name)
        self.column_names = self._get_column_names()
        self.freshness_suggestions = None
        self.flow: list[BaseSuggestion] = []
        self.current_suggestion_index = 0
        self.filter_sql: str | None = None
        self.has_sodacl = False

    def create_flow(self):
        selected_suggestions = SuggestionFilter(event_tracker=self.event_tracker, dataset_name=self.dataset_name).ask()
        if not selected_suggestions:
            return
        suggestion_cls_maps = {
            "schema_check": SchemaCheckSuggestion,
            "row_count": RowCountCheckSuggestion,
            "partition": PartitionSuggestion,
            "freshness_check": FreshnessCheckSuggestion,
            "validity_check": ValidityCheckSuggestion,
            "missing_value_check": MissingValueCheckSuggestion,
            "duplicate_value_check": DuplicateValueCheckSuggestion,
            "sodacl_runner": SodaCLRunner,
        }
        filtered_column_names = self._get_filtered_column_names(selected_suggestions=selected_suggestions)
        # Add SodaCLRunner to the end of the flow
        selected_suggestions.append("sodacl_runner")

        total_steps = len(selected_suggestions)
        for i, suggestion_key in enumerate(selected_suggestions):
            suggestion_cls = suggestion_cls_maps.get(suggestion_key)

            if suggestion_key == "freshness_check" or suggestion_key == "partition":
                self.freshness_suggestions = self._get_or_create_freshness_suggestions()
                suggestion_obj = suggestion_cls(
                    event_tracker=self.event_tracker,
                    dataset_name=self.dataset_name,
                    freshness_suggestion=self.freshness_suggestions,
                )
            elif suggestion_key == "validity_check":
                potential_validity_checks = self._get_potential_validity_checks()
                suggestion_obj = suggestion_cls(
                    event_tracker=self.event_tracker,
                    dataset_name=self.dataset_name,
                    potential_validity_checks=potential_validity_checks,
                )
            elif suggestion_key == "duplicate_value_check" or suggestion_key == "missing_value_check":
                suggestion_obj = suggestion_cls(
                    event_tracker=self.event_tracker,
                    dataset_name=self.dataset_name,
                    column_names=filtered_column_names,
                )
            elif suggestion_key == "sodacl_runner":
                suggestion_obj = suggestion_cls(
                    event_tracker=self.event_tracker, dataset_name=self.dataset_name, scan=self.adaptor.scan
                )
            else:
                suggestion_obj = suggestion_cls(event_tracker=self.event_tracker, dataset_name=self.dataset_name)
            suggestion_obj.set_step(step=i + 1)
            suggestion_obj.set_total_steps(total_steps)
            self.flow.append(suggestion_obj)

    def run_flow(self):
        while self.current_suggestion_index < len(self.flow):
            suggestion = self.flow[self.current_suggestion_index]
            # Handle SodaCLRunner separately
            if isinstance(suggestion, SodaCLRunner):
                suggestion.set_soda_cl(self._get_sodacl_content())
                if suggestion.soda_cl is not None:
                    suggestion.set_file_name(self.soda_cl_generator.file_name)
                    self.has_sodacl = True
            suggestion_answer = self._ask_suggestion_questions(suggestion)
            if suggestion_answer == SKIPPED_STEP_MESSAGE:
                # The step has been skipped, so we can remove it from the flow
                self.flow.pop(self.current_suggestion_index)
            elif suggestion_answer == PREVIOUS_STEP_MESSAGE:
                self.current_suggestion_index = max(0, self.current_suggestion_index - 1)
            else:
                self.current_suggestion_index += 1
                if isinstance(suggestion, PartitionSuggestion) and suggestion.partition_column is not None:
                    self._set_filter_sql(
                        partition_column=suggestion.partition_column,
                        n_days_to_partition_by=suggestion.n_days_to_partition_by,
                    )

    def write_flow_to_sodacl(self):
        soda_cl_content = self._get_sodacl_content()
        self.soda_cl_generator.set_file_content(content=soda_cl_content)
        self.soda_cl_generator.write_soda_cl_to_file()

    def show_instuctions(self):
        instruction_configs = get_raw_flow_configurations()["instructions"]
        if not self.soda_cl_generator.file_content:
            na_message = instruction_configs["na_message"]
            print_warning_panel(na_message)
            return

        sodacl_file_path = self.soda_cl_generator.file_path
        config_file_path = self.adaptor.configuration_f_path
        data_source_name = self.adaptor.data_source_name

        instruction_text = instruction_configs["instruction"]
        soda_scan_command = f"soda scan -c {config_file_path} -d {data_source_name} {sodacl_file_path}"
        formatted_soda_scan_command = f"[{SODA_PRIMARY_GREEN}][bold]{soda_scan_command}[/bold][/{SODA_PRIMARY_GREEN}]"
        formatted_sodacl_file_path = (
            f"[{SODA_PRIMARY_GREEN}][bold][link=file://{sodacl_file_path}]"
            f"{sodacl_file_path}[/link][/bold][/{SODA_PRIMARY_GREEN}]"
        )
        instructions_text = instruction_text.format(
            sodacl_file_path=formatted_sodacl_file_path,
            soda_scan_command=formatted_soda_scan_command,
        )
        self.console.print(
            Panel(
                instructions_text,
                title=instruction_configs["title"],
                title_align="left",
                border_style=SODA_PRIMARY_GREEN,
            )
        )

    def _set_filter_sql(self, partition_column: str, n_days_to_partition_by: int):
        self.filter_sql = self.adaptor.get_filter_sql(
            partition_column=partition_column,
            n_days_to_partition_by=n_days_to_partition_by,
        )

    def _get_sodacl_content(self) -> str | None:
        sodacl_content = ""
        for suggestion in self.flow:
            if not isinstance(suggestion, SodaCLRunner):
                sodacl_content += suggestion.get_soda_cl()
        if sodacl_content == "":
            return None
        sodacl_content = self.soda_cl_generator.merge_file_contents(content=sodacl_content, filter_sql=self.filter_sql)
        return sodacl_content

    def _ask_suggestion_questions(self, suggestion: BaseSuggestion):
        try:
            return suggestion.ask()
        except KeyboardInterrupt:
            exit_decision = q.confirm("Are you sure to exit?", style=self.custom_q_style).unsafe_ask()
            soda_cl_content = self._get_sodacl_content()
            if exit_decision is True and soda_cl_content is not None:
                save_file_decision = q.confirm(
                    "Do you want to save the currently generated " f"SodaCL into {self.soda_cl_generator.file_path}?",
                    style=self.custom_q_style,
                ).unsafe_ask()
                if save_file_decision:
                    self.write_flow_to_sodacl()
                    print_info_panel(f"Successfully saved SodaCL file to {self.soda_cl_generator.file_path}")
                raise KeyboardInterrupt
            elif exit_decision and not self.soda_cl_generator.file_content:
                raise KeyboardInterrupt
            else:
                self._ask_suggestion_questions(suggestion)

    def _get_dataset_name(self, dataset_name: str | None) -> str:
        dataset_selected_in_flow = False
        if dataset_name is None:
            dataset_selected_in_flow = True
            dataset_names: list[str] = list(self.datasets_metadata.datasets.keys())
            dataset_name_dict = get_raw_flow_configurations()["dataset_names"]
            dataset_name_config = DatasetNameConfigs(**dataset_name_dict)
            print_header(dataset_name_config)
            dataset_name: str = q.autocomplete(
                dataset_name_config.instruction,
                choices=dataset_names,
                validate=lambda x: x in dataset_names,
                complete_style=CompleteStyle.MULTI_COLUMN,
                style=self.custom_q_style,
            ).unsafe_ask()

            self.event_tracker.send_tracking_event(
                event_name="dataset_selected_in_flow",
                step_name="get_dataset_name",
                user_choice=dataset_selected_in_flow,
            )
        return dataset_name

    def _get_single_dataset_metadata(self, dataset_name: str) -> Dataset:
        dataset = self.datasets_metadata.datasets.get(dataset_name)
        if dataset is None:
            raise InvalidDatasetNameException(
                f"[red bold]{dataset_name}[/red bold] dataset does not exist in [red bold]{self.adaptor.data_source_name}[/red bold]"
            )
        return dataset

    def _get_multiple_dataset_metadata(self, dataset_name: None | str = None) -> DatasetsMetadata:
        reuse_metadata_decision = False
        # Check if the directory exists
        if not os.path.exists(SODA_DIR):
            # Create the directory if it doesn't exist
            os.makedirs(SODA_DIR)

        # Read from the directory
        dataset_metadata_path = SODA_DIR / f"{self.adaptor.data_source_name}_dataset_metadata.json"
        if os.path.exists(dataset_metadata_path):
            # Ask user whether they want to reuse the existing metadata
            dataset_name_dict = get_raw_flow_configurations()["dataset_metadata"]
            dataset_metadata_config = DatasetMetadataConfigs(**dataset_name_dict)
            message = dataset_metadata_config.instruction.format(data_source_name=self.adaptor.data_source_name)
            reuse_metadata_decision = q.confirm(message=message, style=self.custom_q_style).unsafe_ask()
            if reuse_metadata_decision:
                json_content = read_json(dataset_metadata_path)
                self.event_tracker.send_tracking_event(
                    event_name="use_cached_datasource_metadata",
                    step_name="pre_suggestions",
                    user_choice=True,
                )
                return DatasetsMetadata(**json_content)
        datasets_metadata = self._create_dataset_metadata(dataset_name)
        write_json(path=dataset_metadata_path, content=datasets_metadata.dict(by_alias=True))
        self.event_tracker.send_tracking_event(
            event_name="use_cached_datasource_metadata",
            step_name="pre_suggestions",
            user_choice=True if reuse_metadata_decision else False,
        )
        return datasets_metadata

    def _create_dataset_metadata(self, dataset_name=None) -> DatasetsMetadata:
        with self.console.status("Getting dataset metadata...", spinner="moon"):
            self._execute_sodacl_discover_datasets(included_dataset=dataset_name)
            metadata = self.adaptor.scan.scan_results.get("metadata")
            data_source_name = self.adaptor.data_source_name
            if not metadata and dataset_name:
                raise InvalidDatasetNameException(
                    f"[red bold]{dataset_name}[/red bold] dataset does not"
                    f" exist in [red bold]{data_source_name}[/red bold]"
                )

            if not metadata and not dataset_name:
                raise NoDatasetException(f"[red bold]{data_source_name}[/red bold] does not have any dataset")
            datasets = {}
            for dataset_metadata in metadata:
                dataset_name = dataset_metadata.get("table")
                columns = []
                for column_metadata in dataset_metadata.get("schema"):
                    column_name = column_metadata.get("columnName")
                    data_type = column_metadata.get("sourceDataType")
                    column = Column(name=column_name, data_type=data_type)
                    columns.append(column)
                dataset = Dataset(name=dataset_name, columns=columns)
                datasets[dataset_name] = dataset
            self.console.print(
                ":white_check_mark: All dataset and column "
                f"names are fetched successfully from '{data_source_name}'!"
            )
            return DatasetsMetadata(datasets=datasets)

    def _execute_sodacl_discover_datasets(self, included_dataset):
        # parse jinja template having dataset_name as a variable
        if included_dataset is None:
            pattern = "%"
        else:
            pattern = included_dataset
        discover_datasets_sodacl_yaml_str = parse_sodacl_jinja_template(
            "discover_datasets.yaml", included_dataset=pattern
        )
        self.adaptor.scan.add_sodacl_yaml_str(discover_datasets_sodacl_yaml_str)
        self.adaptor.scan.execute()

    def _get_column_names(self):
        return [column.name for column in self.datasets_metadata.datasets[self.dataset_name].columns]

    def _get_filtered_column_names(self, selected_suggestions: list[str]) -> list[str]:
        column_based_selected_suggestions = []
        for suggestion in selected_suggestions:
            if suggestion == "missing_value_check":
                column_based_selected_suggestions.append("Missing Value Check")
            elif suggestion == "duplicate_value_check":
                column_based_selected_suggestions.append("Duplicate Values Check")
        if not column_based_selected_suggestions:
            return []
        column_based_selected_suggestions = [
            f"[#126ab3]{check_name}[/]" for check_name in column_based_selected_suggestions
        ]
        filtered_column_names = ColumnFilter(
            event_tracker=self.event_tracker,
            dataset_name=self.dataset_name,
            column_names=self.column_names,
            column_based_selected_suggestions=column_based_selected_suggestions,
        ).ask()
        return filtered_column_names

    def _get_or_create_freshness_suggestions(self):
        if self.freshness_suggestions is not None:
            return self.freshness_suggestions

        date_dtypes = [
            "date",
            "datetime",
            "timestamp",
            "timestamp without time zone",
            "timestamp with time zone",
        ]
        date_column_names = []
        dataset_name = self.dataset_metadata.name

        for column in self.dataset_metadata.columns:
            if column.data_type.lower() in date_dtypes:
                date_column_names.append(column.name)

        if len(date_column_names) == 0:
            return []
        df_sample = self.adaptor.get_sample_pd_dataframe(
            dataset_name=dataset_name, column_names=date_column_names, sample_size=int(1e5)
        )
        df_sample = df_sample.apply(pd.to_datetime)
        freshness_detector = FreshnessColumnDetector().detect(df_sample)
        soda_cl_suggestions = []
        for freshness_attr in freshness_detector.freshness_attributes:
            column_name = freshness_attr.col_name
            threshold = freshness_attr.threshold.soda_cl_threshold_str
            soda_cl = (column_name, threshold)
            soda_cl_suggestions.append(soda_cl)
        return soda_cl_suggestions

    def _get_potential_validity_checks(self) -> list[tuple[str, str]]:
        text_columm_names = set()
        for column in self.dataset_metadata.columns:
            if column.data_type in self.adaptor.text_dtypes:
                text_columm_names.add(column.name)
            for text_dtype in self.adaptor.text_dtypes:
                if text_dtype in column.data_type:
                    text_columm_names.add(column.name)

        if len(text_columm_names) == 0:
            return []
        max_sample_size = int(1e3)
        df_sample = self.adaptor.get_sample_pd_dataframe(
            dataset_name=self.dataset_name,
            column_names=list(text_columm_names),
            sample_size=max_sample_size,
        ).astype(str)
        dtype_regex = self.adaptor.default_formats
        scope_semantic_dtype_hierarchy = self._parse_semantic_dtype_hierarcy_config()
        n_rows = len(df_sample)
        validity_checks = []
        for column_name in text_columm_names:
            series = df_sample[column_name].dropna().str.strip()
            series_length = len(series)
            # If more than 1/3 of the values are null, skip the column
            if series_length >= n_rows / 3:
                threshold = int(0.95 * series_length)
                for semantic_dtype in scope_semantic_dtype_hierarchy:
                    # More than 95% of the values should match the regex
                    semantic_dtype_regex = dtype_regex[semantic_dtype]
                    if series.str.match(semantic_dtype_regex).sum() >= threshold:
                        validity_checks.append((column_name, semantic_dtype))
        return validity_checks

    @staticmethod
    def _parse_semantic_dtype_hierarcy_config():
        sodacl_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "configurations")
        with open(os.path.join(sodacl_dir, "validity_check_hierarchy.yaml")) as f:
            yaml = YAML(typ="safe")
            semantic_dtypes = yaml.load(f)["dtype_hierarchy"]
            ordered_semantic_dtypes = []

            for _, dtypes in semantic_dtypes.items():
                ordered_semantic_dtypes += dtypes
        return ordered_semantic_dtypes
