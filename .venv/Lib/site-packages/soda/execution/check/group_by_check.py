from __future__ import annotations

import copy

from soda.cloud.historic_descriptor import (
    HistoricChangeOverTimeDescriptor,
    HistoricCheckResultsDescriptor,
    HistoricMeasurementsDescriptor,
)
from soda.execution.check.anomaly_metric_check import (
    HISTORIC_MEASUREMENTS_LIMIT,
    KEY_HISTORIC_CHECK_RESULTS,
    KEY_HISTORIC_MEASUREMENTS,
    AnomalyMetricCheck,
)
from soda.execution.check.change_over_time_metric_check import (
    KEY_HISTORIC_METRIC_AGGREGATE,
    ChangeOverTimeMetricCheck,
)
from soda.execution.check.check import Check
from soda.execution.check_outcome import CheckOutcome
from soda.execution.check_type import CheckType
from soda.execution.metric.metric import Metric
from soda.execution.partition import Partition

GROUP_BY_RESULTS = "group_by_results"


class GroupByCheck(Check):
    def __init__(
        self,
        check_cfg: GroupByCheckCfg,
        data_source_scan: DataSourceScan,
        partition: Partition,
    ):
        super().__init__(
            check_cfg=check_cfg,
            data_source_scan=data_source_scan,
            partition=partition,
            column=None,
        )

        self.check_value = None
        self.check_type = CheckType.LOCAL

        from soda.execution.metric.group_by_metric import GroupByMetric

        group_by_metric = data_source_scan.resolve_metric(
            GroupByMetric(
                data_source_scan=self.data_source_scan,
                partition=partition,
                query=self.check_cfg.query,
                check=self,
            )
        )
        self.metrics[GROUP_BY_RESULTS] = group_by_metric

    def evaluate(self, metrics: dict[str, Metric], historic_values: dict[str, object]):
        query_results = metrics[GROUP_BY_RESULTS].value
        group_limit = self.check_cfg.group_limit

        if len(query_results) > group_limit:
            raise Exception(
                f"Total number of groups {len(query_results)} exceeds configured group limit: {group_limit}"
            )

        fields = list(self.check_cfg.fields)
        # validate if configured fields are part of query_results
        all_keys = set().union(*(qr.keys() for qr in query_results))
        if not set(fields).issubset(all_keys):
            raise Exception(f"configured fields: {fields} are not found in query results.")

        group_check_cfgs = self.check_cfg.check_cfgs
        groups = [tuple(map(qr.get, fields)) for qr in query_results]

        group_checks = []

        for group in groups:
            for gcc in group_check_cfgs:
                group_name = f"{','.join(str(v) for v in group)}"
                config = copy.deepcopy(gcc)

                if not config.name:
                    config.name = config.source_line
                config.name += f" [{group_name}]"

                if not config.source_configurations:
                    config.source_configurations = {}

                config.source_configurations["group_value"] = f"[{group_name}]"
                column = ",".join(fields)
                # TODO metrics are created in check constructors and resolved. This is unnecessary and confusing, we replace those metrics below.
                # TODO: Add the group by query to config.source_configurations to be consistent with how other checks handle identity.
                gc = Check.create(
                    check_cfg=config, data_source_scan=self.data_source_scan, partition=self.partition, column=column
                )

                result = next(filter(lambda qr: tuple(map(qr.get, fields)) == group, query_results))
                if result is not None:
                    gc.check_value = result[config.metric_name]
                    metric = Metric(
                        self.data_source_scan,
                        self.partition,
                        column=None,
                        name=f"{config.source_line} [{group_name}]",
                        check=None,
                        identity_parts=[],
                    )

                    historic_values = {}
                    if gc.historic_descriptors:
                        # Override historic descriptors with group by ones
                        if isinstance(gc, AnomalyMetricCheck):
                            gc.historic_descriptors[KEY_HISTORIC_MEASUREMENTS] = HistoricMeasurementsDescriptor(
                                metric_identity=metric.identity,
                                limit=HISTORIC_MEASUREMENTS_LIMIT,
                            )
                            gc.historic_descriptors[KEY_HISTORIC_CHECK_RESULTS] = HistoricCheckResultsDescriptor(
                                check_identity=gc.create_identity(), limit=HISTORIC_MEASUREMENTS_LIMIT
                            )
                        elif isinstance(gc, ChangeOverTimeMetricCheck):
                            gc.historic_descriptors[KEY_HISTORIC_METRIC_AGGREGATE] = HistoricChangeOverTimeDescriptor(
                                metric_identity=metric.identity,
                                change_over_time_cfg=gc.check_cfg.change_over_time_cfg,
                            )

                        # Get the historic values.
                        for hd_key, hd in gc.historic_descriptors.items():
                            historic_values[
                                hd_key
                            ] = self.data_source_scan.scan._get_historic_data_from_soda_cloud_metric_store(hd)

                    metric.set_value(gc.check_value)
                    self.data_source_scan.scan._add_metric(metric)
                    gc.metrics = {config.metric_name: metric}
                    gc.evaluate(metrics=gc.metrics, historic_values=historic_values)

                    cloud_group_attr = {
                        "group": {
                            "identity": self.create_identity(with_datasource=True, with_filename=True),
                            "name": self.name,
                            "distinctLabel": group_name,
                        }
                    }
                    gc.cloud_dict.update(cloud_group_attr)
                    gc.dict.update(cloud_group_attr)

                (are_attributes_valid, check_attributes) = self.validate_attributes(gc)
                if are_attributes_valid is False:
                    self.data_source_scan.scan.invalid_checks.append(gc)
                else:
                    gc.attributes = check_attributes
                    group_checks.append(gc)

        self.data_source_scan.scan._checks.extend(group_checks)

        if all(gc.outcome == CheckOutcome.PASS for gc in group_checks):
            self.outcome = CheckOutcome.PASS
        elif any(gc.outcome == CheckOutcome.FAIL for gc in group_checks):
            self.outcome = CheckOutcome.FAIL
        else:
            self.outcome = CheckOutcome.PASS

    def validate_attributes(self, gc):
        are_attributes_valid = True
        scan = self.data_source_scan.scan
        if gc.check_cfg.source_configurations:
            check_attributes = {
                scan.jinja_resolve(k): scan.jinja_resolve(v)
                for k, v in gc.check_cfg.source_configurations.get("attributes", {}).items()
            }

            if scan._configuration.soda_cloud:
                # Validate attributes if Cloud is available
                if check_attributes:
                    from soda.common.attributes_handler import AttributeHandler

                    attribute_handler = AttributeHandler(scan._logs)
                    attributes_schema = scan._configuration.soda_cloud.get_check_attributes_schema()

                    check_attributes, are_attributes_valid = attribute_handler.validate(
                        check_attributes, attributes_schema
                    )

            return (are_attributes_valid, check_attributes)

    def get_cloud_diagnostics_dict(self) -> dict:
        group_by_diagnostics = {}
        return group_by_diagnostics

    def get_log_diagnostic_lines(self) -> list[str]:
        diagnostics_texts: list[str] = []
        return diagnostics_texts
