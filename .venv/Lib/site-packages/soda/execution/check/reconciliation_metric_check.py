from __future__ import annotations

from numbers import Number

from soda.common.exceptions import CheckConfigError
from soda.execution.check.check import Check
from soda.execution.check_outcome import CheckOutcome
from soda.execution.metric.derived_metric import DERIVED_METRIC_NAMES
from soda.execution.metric.metric import Metric
from soda.execution.metric.user_defined_numeric_metric import UserDefinedNumericMetric
from soda.sodacl.reconciliation_metric_check_cfg import ReconciliationMetricCheckCfg

KEY_CHECK_VALUE = "check_value"


class ReconciliationMetricCheck(Check):
    def __init__(
        self,
        check_cfg: ReconciliationMetricCheckCfg,
        data_source_scan: DataSourceScan,
        partition: Partition | None = None,
        column: Column | None = None,
    ):
        from soda.execution.table import Table

        check_cfg: ReconciliationMetricCheckCfg = check_cfg
        metric_name = check_cfg.metric_name

        super().__init__(check_cfg=check_cfg, data_source_scan=data_source_scan, column=column, partition=partition)

        self.historic_descriptor = None
        self.metric_store_request = None

        self.check_value: float | None = None
        self.formula_values: dict | None = None
        self.historic_diff_values: dict | None = None

        from soda.execution.metric.derived_metric import DerivedMetric
        from soda.execution.metric.numeric_query_metric import NumericQueryMetric

        is_user_defined_metric_query = (
            check_cfg.metric_query is not None
            or check_cfg.source_metric_query is not None
            or check_cfg.target_metric_query is not None
        )

        is_user_defined_metric_expression = (
            check_cfg.metric_expression is None
            or check_cfg.source_metric_expression is not None
            or check_cfg.target_metric_expression is not None
        )

        scan = data_source_scan.scan

        source_config = check_cfg.reconciliation_configurations["source"]
        target_config = check_cfg.reconciliation_configurations["target"]

        source_data_source_scan = scan._get_or_create_data_source_scan(source_config["datasource"])
        source_table: Table = source_data_source_scan.get_or_create_table(source_config["dataset"])
        # TODO get partition name from config
        source_partition = source_table.get_or_create_partition(None)
        source_filter = source_config.get("filter")

        target_data_source_scan = scan._get_or_create_data_source_scan(target_config["datasource"])
        target_table: Table = target_data_source_scan.get_or_create_table(
            target_config["dataset"]
        )  # TODO get partition name from config
        target_partition = target_table.get_or_create_partition(None)
        target_filter = target_config.get("filter")

        self.metric_values: dict = {}

        if not is_user_defined_metric_query:
            is_derived = metric_name in DERIVED_METRIC_NAMES
            source_metric_args = {
                "data_source_scan": source_data_source_scan,
                "partition": source_partition,
                "column": self.column,
                "metric_name": metric_name,
                "metric_args": check_cfg.metric_args,
                "filter": source_filter,
                "check_missing_and_valid_cfg": check_cfg.missing_and_valid_cfg,
                "column_configurations_cfg": self.column.column_configurations_cfg if self.column else None,
                "aggregation": check_cfg.source_metric_expression
                if is_user_defined_metric_expression
                else check_cfg.metric_expression,
                "check": self,
            }
            target_metric_args = {
                "data_source_scan": target_data_source_scan,
                "partition": target_partition,
                "column": self.column,
                "metric_name": metric_name,
                "metric_args": check_cfg.metric_args,
                "filter": target_filter,
                "check_missing_and_valid_cfg": check_cfg.missing_and_valid_cfg,
                "column_configurations_cfg": self.column.column_configurations_cfg if self.column else None,
                "aggregation": check_cfg.target_metric_expression
                if is_user_defined_metric_expression
                else check_cfg.metric_expression,
                "check": self,
            }

            if not is_derived:
                source_metric = NumericQueryMetric(
                    **source_metric_args,
                )
                target_metric = NumericQueryMetric(**target_metric_args)
            else:
                source_metric = DerivedMetric(**source_metric_args)
                target_metric = DerivedMetric(**target_metric_args)
        else:
            if data_source_scan.data_source.is_supported_metric_name(metric_name):
                self.logs.warning(
                    f"User defined metric {metric_name} is a data_source supported metric. Please, choose a different name for the metric.",
                    location=check_cfg.location,
                )

            source_metric_query = check_cfg.source_metric_query or check_cfg.metric_query
            target_metric_query = check_cfg.target_metric_query or check_cfg.metric_query

            source_metric = UserDefinedNumericMetric(
                data_source_scan=source_data_source_scan,
                check_name=check_cfg.source_line,
                sql=source_metric_query,
                check=self,
            )
            target_metric = UserDefinedNumericMetric(
                data_source_scan=target_data_source_scan,
                check_name=check_cfg.source_line,
                sql=target_metric_query,
                check=self,
            )

        self.metrics["source_metric"] = source_data_source_scan.resolve_metric(source_metric)
        self.metrics["target_metric"] = target_data_source_scan.resolve_metric(target_metric)

    def evaluate(self, metrics: dict[str, Metric], historic_values: dict[str, object]):
        try:
            is_percentage = self.check_threshold_is_percentage(
                self.check_cfg.warn_threshold_cfg, self.check_cfg.fail_threshold_cfg
            )

        except CheckConfigError as e:
            self.logs.warning(e, location=self.check_cfg.location)
            return

        diff_value = abs(round(metrics["source_metric"].value - metrics["target_metric"].value, 2))
        self.check_value = diff_value

        self.metric_values = {
            f"source_{metrics['source_metric'].name}": metrics["source_metric"].value,
            f"target_{metrics['target_metric'].name}": metrics["target_metric"].value,
            "diff_value": diff_value,
        }

        if is_percentage:
            try:
                diff_percentage = (
                    abs(round((diff_value / metrics["source_metric"].value) * 100, 2)) if diff_value != 0 else 0
                )
            except ZeroDivisionError:
                diff_percentage = None
                self.logs.error(
                    f"Unable to calculate diff percentage for check '{self.name}'. Source metric value is zero causing division by zero error.",
                    location=self.check_cfg.location,
                )

            self.metric_values["diff_percentage"] = diff_percentage

            self.check_value = diff_percentage

        self.set_outcome_based_on_check_value()

    def get_log_diagnostic_dict(self) -> dict:
        log_diagnostics = {"check_value": self.check_value, **self.metric_values}
        if self.formula_values:
            log_diagnostics.update(self.formula_values)
        return log_diagnostics

    def set_outcome_based_on_check_value(self):
        metric_check_cfg: ReconciliationMetricCheckCfg = self.check_cfg
        if self.check_value is not None and metric_check_cfg.has_threshold():
            metric_check_cfg.resolve_thresholds(self.data_source_scan.scan.jinja_resolve)
            if isinstance(self.check_value, Number):
                if metric_check_cfg.fail_threshold_cfg and metric_check_cfg.fail_threshold_cfg.is_bad(self.check_value):
                    self.outcome = CheckOutcome.FAIL
                elif metric_check_cfg.warn_threshold_cfg and metric_check_cfg.warn_threshold_cfg.is_bad(
                    self.check_value
                ):
                    self.outcome = CheckOutcome.WARN
                else:
                    self.outcome = CheckOutcome.PASS
            else:
                hint = (
                    " Is your column text based? The valid format config only works on text columns."
                    if metric_check_cfg.missing_and_valid_cfg and metric_check_cfg.missing_and_valid_cfg.valid_format
                    else ""
                )
                self.logs.error(
                    f"Cannot evaluate check: Expected a numeric value, but was {self.check_value}.{hint}",
                    location=self.check_cfg.location,
                )

    def get_cloud_diagnostics_dict(self) -> dict:
        metric_check_cfg: ReconciliationMetricCheckCfg = self.check_cfg

        cloud_diagnostics = super().get_cloud_diagnostics_dict()

        if metric_check_cfg.fail_threshold_cfg is not None:
            cloud_diagnostics["fail"] = metric_check_cfg.fail_threshold_cfg.to_soda_cloud_diagnostics_json()
        if metric_check_cfg.warn_threshold_cfg is not None:
            cloud_diagnostics["warn"] = metric_check_cfg.warn_threshold_cfg.to_soda_cloud_diagnostics_json()

        diagnostic_text = ""
        for key, value in self.metric_values.items():
            diagnostic_text += f"{key.replace('_', ' ').title()}, {value}\n"

        diagnostics_block = {
            "type": "csv",
            "title": "Diagnostics",
            "text": f"Key, Value\n{diagnostic_text}",
        }
        cloud_diagnostics["blocks"].append(diagnostics_block)

        return cloud_diagnostics

    def get_cloud_dict(self):
        d = super().get_cloud_dict()

        if "dataSource" in d:
            del d["dataSource"]

        if "table" in d:
            del d["table"]

        d["dataSources"] = [
            {
                "dataSource": self.check_cfg.reconciliation_configurations["source"]["datasource"],
                "table": self.check_cfg.reconciliation_configurations["source"]["dataset"],
                "qualifier": "source",
            },
            {
                "dataSource": self.check_cfg.reconciliation_configurations["target"]["datasource"],
                "table": self.check_cfg.reconciliation_configurations["target"]["dataset"],
                "qualifier": "target",
            },
        ]

        d["group"] = {
            "identity": self.check_cfg.reconciliation_configurations["group_identity"],
            "name": self.check_cfg.reconciliation_configurations["label"],
            "distinctLabel": self.name,
            "type": "reconciliation",
        }

        return d

    def identity_datasource_part(self) -> list[str]:
        return [
            self.check_cfg.reconciliation_configurations["source"]["datasource"],
            self.check_cfg.reconciliation_configurations["source"]["dataset"],
            self.check_cfg.reconciliation_configurations["target"]["datasource"],
            self.check_cfg.reconciliation_configurations["target"]["dataset"],
        ]
