from __future__ import annotations

from soda.cloud.soda_cloud import SodaCloud
from soda.sampler.sample_ref import SampleRef


class CloudCheckMixin:
    def get_cloud_dict(self):
        from soda.execution.column import Column
        from soda.execution.partition import Partition

        self.cloud_dict.update(
            {
                # See https://sodadata.atlassian.net/browse/CLOUD-1143
                "identity": self.create_identity(with_datasource=True, with_filename=True),
                "identities": self.create_identities(),
                "name": self.name,
                "type": self.cloud_check_type,
                "definition": self.create_definition(),
                "resourceAttributes": self._format_attributes(),
                "location": self.check_cfg.location.get_cloud_dict(),
                "dataSource": self.data_source_scan.data_source.data_source_name,
                "table": Partition.get_table_name(self.partition),
                # "filter": Partition.get_partition_name(self.partition), TODO: re-enable once backend supports the property.
                "column": Column.get_partition_name(self.column),
                "metrics": [metric.identity for metric in self.metrics.values()],
                "outcome": self.outcome.value if self.outcome else None,
                "diagnostics": self.get_cloud_diagnostics_dict(),
                "source": "soda-library",
            }
        )
        # Update dict if automated monitoring is running
        if self.archetype is not None:
            self.cloud_dict.update({"archetype": self.archetype})

        # Update dict if check is skipped and we want to push reason to cloud
        if self.outcome_reasons:
            self.cloud_dict.update({"outcomeReasons": self.outcome_reasons})
        return self.cloud_dict

    def get_cloud_diagnostics_dict(self) -> dict:
        cloud_diagnostics = {
            "blocks": [],
            "value": self.check_value if hasattr(self, "check_value") else None,
        }

        if self.failed_rows_sample_ref and self.failed_rows_sample_ref.type != SampleRef.TYPE_NOT_PERSISTED:
            if self.cloud_check_type == "generic":
                queries = self._get_all_related_queries()
                has_analysis_block = False
                sample_ref_block = self.failed_rows_sample_ref.get_cloud_diagnostics_block()

                for query in queries:
                    if query.failing_sql and query.passing_sql and sample_ref_block:
                        has_analysis_block = True
                        total_failing_rows = self.check_value

                        # TODO: temporary workaround until derived checks are refactored.
                        derived_failing_rows_formula_values = [
                            "invalid_count",
                            "duplicate_count",
                            "missing_count",
                        ]
                        if hasattr(self, "formula_values") and self.formula_values:
                            for derived_value in derived_failing_rows_formula_values:
                                if derived_value in self.formula_values:
                                    total_failing_rows = self.formula_values[derived_value]

                        rca_block = {
                            "type": "failedRowsAnalysis",
                            "title": "Failed Rows Analysis",
                            "file": sample_ref_block["file"],
                            "failingRowsQueryName": f"{query.query_name}.failing_sql",
                            "passingRowsQueryName": f"{query.query_name}.passing_sql",
                            "totalFailingRows": total_failing_rows,
                            "sampleRowCount": sample_ref_block["file"]["storedRowCount"],
                        }
                        cloud_diagnostics["blocks"].append(rca_block)

                    # TODO: This should be a second failed rows file, refactor failed rows to support multiple files.
                    if (
                        query.failing_rows_sql_aggregated
                        and hasattr(query, "aggregated_failed_rows_data")
                        and query.aggregated_failed_rows_data
                    ):
                        text = f'{",".join(query.metric.metric_args)},frequency'

                        for row in query.aggregated_failed_rows_data:
                            row_str = f'\n{",".join(map(str, row))}'
                            if len(text) + len(row_str) < SodaCloud.CSV_TEXT_MAX_LENGTH:
                                text += row_str

                        aggregate_rows_block = {
                            "type": "csv",
                            "title": "Failed Rows Aggregate",
                            "text": text,
                        }
                        cloud_diagnostics["blocks"].append(aggregate_rows_block)

                if not has_analysis_block:
                    cloud_diagnostics["blocks"].append(self.failed_rows_sample_ref.get_cloud_diagnostics_block())

            else:
                cloud_diagnostics["blocks"].append(self.failed_rows_sample_ref.get_cloud_diagnostics_block())

        return cloud_diagnostics
